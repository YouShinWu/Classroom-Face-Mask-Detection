{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person_with_mask', 'person_no_mask']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WUYOUS~1\\AppData\\Local\\Temp/ipykernel_3144/3558951943.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#讀取畫面長、寬、通道\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m#將影像預處理，參數依序為：影像、正規化、縮放尺寸、RB通道交換\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m608\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m608\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswapRB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 引入 time 模組\n",
    "import time\n",
    "# 開始測量時間\n",
    "start = time.time()\n",
    "# fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "# writer = cv2.VideoWriter('./five_people_detection.mp4', fourcc, 30,\n",
    "# \t(800, 600), True)\n",
    "#讀取權重及神經網路結構建立神經網路\n",
    "net = cv2.dnn.readNet('./yolov4_training_final.weights','./yolov4_test.cfg')\n",
    "#類別名稱\n",
    "classes = []\n",
    "#讀取類別名稱txt\n",
    "with open('./classes.txt','r')as f:\n",
    "    classes = f.read().splitlines()\n",
    "print(classes)\n",
    "color1 = (255, 255, 0)#配置預測框顏色\n",
    "#無限迴圈\n",
    "while True:\n",
    "    #讀取要辨識的圖片\n",
    "    video=cv2.VideoCapture('./videoData/five_people.mp4')\n",
    "    img = video.read()\n",
    "    #讀取畫面長、寬、通道\n",
    "    height, width, channel =img.shape\n",
    "    #將影像預處理，參數依序為：影像、正規化、縮放尺寸、RB通道交換\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255, (608,608), swapRB=True)\n",
    "    #將影像輸入神經網路\n",
    "    net.setInput(blob)\n",
    "    #列出YOLO神經網路輸出層的名稱，YOLOv4有3個輸出層\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    #將這幾層的輸出結果儲存，為多個5+classes維的數組\n",
    "    #5+classes維是由預測框的中心點參數x,y、預測框的長寬參數w,h、\n",
    "    #存在物件的信心度和所屬各類別的信心度所組成\n",
    "    layerOutputs = net.forward(output_layers_names)\n",
    "    \n",
    "    boxes = [] #暫存預測框的參數用\n",
    "    confidences = [] #暫存存在物件信心度用\n",
    "    class_ids = [] #暫存分類編號用\n",
    "    #篩選預測框及分類閥值，輸出層有三個，外迴圈跑三次\n",
    "    #內迴圈次數依據預測框的數量\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            #找尋信心度最高的分類\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            #篩選閥值及儲存框參數\n",
    "            if confidence > 0.6:\n",
    "                #預測框參數的範圍皆為0~1\n",
    "                #需做比例尺轉換\n",
    "                center_x = int(detection[0]*width)\n",
    "                center_y = int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "                x = int(center_x - w/2)\n",
    "                y = int(center_y - h/2)\n",
    "                #暫存預測框參數\n",
    "                boxes.append([x,y,w,h])\n",
    "                #暫存信心度\n",
    "                confidences.append((float(confidence)))\n",
    "                #暫存分類編號\n",
    "                class_ids.append(class_id)\n",
    "    #第二次篩選預測框，參數依序為：預測框參數、存在物件信心度、\n",
    "    #信心度閥值及IoU閥值，\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.6,0.5)\n",
    "        \n",
    "    #畫面標示文字的字體設定，此為小號無襯線字體\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    #暫存預測框顏色用\n",
    "    colors = []\n",
    "    #賦予每個預測框顏色\n",
    "    for i in range(len(boxes)):\n",
    "       colors=np.append(colors,[0,0,255])\n",
    "    colors=np.reshape(colors, (len(boxes),3))\n",
    "    \n",
    "    #將預測框放在影像上\n",
    "    if len(indexes)>0:\n",
    "        for i in indexes.flatten():\n",
    "            x,y,w,h = boxes[i]\n",
    "            #將分類編號對應類別名稱\n",
    "            label = str(classes[class_ids[i]])\n",
    "            print(label)\n",
    "            #取信心度小數點後兩位\n",
    "            confidence = str(round(confidences[i],2))\n",
    "            #預測框顏色\n",
    "            if label == 'person_with_mask':\n",
    "                color=(0,255,0) #Green \n",
    "            else:\n",
    "                color=(0,0,255)\n",
    "            #放置預測框    \n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),color,2)\n",
    "            #若要在預測框上顯示類別名稱與信心度可取消下方註解\n",
    "            # cv2.putText(img, label + \" \" + confidence ,(x,y+20),font,2,(255,255,255),2)\n",
    "    \n",
    "    #左上角顯示預測框數量，參數依序為：影像、位置、字體、縮放比、顏色、粗細\n",
    "    cv2.putText(img, \"Numember of people : \"+str(len(indexes)),(50,50),font,2,(255,50,0),2)\n",
    "    #儲存完成辨識後的圖片\n",
    "    cv2.imshow('img',img)\n",
    "    break\n",
    "# 結束測量\n",
    "end = time.time()\n",
    "\n",
    "# 輸出結果\n",
    "\n",
    "print(\"執行時間：%f 秒\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\WUYOUS~1\\AppData\\Local\\Temp/ipykernel_3144/1111013932.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# determine only the *output* layer names that we need from YOLO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mln\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WUYOUS~1\\AppData\\Local\\Temp/ipykernel_3144/1111013932.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# determine only the *output* layer names that we need from YOLO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mln\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import FPS\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "\n",
    "\n",
    "INPUT_FILE='./vidoData/five_people.mp4'\n",
    "OUTPUT_FILE='output.avi'\n",
    "LABELS_FILE='./classes.txt'\n",
    "CONFIG_FILE='./yolov4_test.cfg'\n",
    "WEIGHTS_FILE='./yolov4_training_final.weights'\n",
    "CONFIDENCE_THRESHOLD=0.6\n",
    "\n",
    "H=None\n",
    "W=None\n",
    "\n",
    "fps = FPS().start()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "writer = cv2.VideoWriter(OUTPUT_FILE, fourcc, 30,\n",
    "\t(800, 600), True)\n",
    "\n",
    "LABELS = open(LABELS_FILE).read().strip().split(\"\\n\")\n",
    "\n",
    "np.random.seed(4)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "\tdtype=\"uint8\")\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(CONFIG_FILE, WEIGHTS_FILE)\n",
    "\n",
    "vs = cv2.VideoCapture(INPUT_FILE)\n",
    "\n",
    "\n",
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "cnt =0\n",
    "while True:\n",
    "\tcnt+=1\n",
    "\tprint (\"Frame number\", cnt)\n",
    "\ttry:\n",
    "\t\t(grabbed, image) = vs.read()\n",
    "\texcept:\n",
    "\t\tbreak\n",
    "\tblob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tif W is None or H is None:\n",
    "\t\t(H, W) = image.shape[:2]\n",
    "\tlayerOutputs = net.forward(ln)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# initialize our lists of detected bounding boxes, confidences, and\n",
    "\t# class IDs, respectively\n",
    "\tboxes = []\n",
    "\tconfidences = []\n",
    "\tclassIDs = []\n",
    "\n",
    "\t# loop over each of the layer outputs\n",
    "\tfor output in layerOutputs:\n",
    "\t\t# loop over each of the detections\n",
    "\t\tfor detection in output:\n",
    "\t\t\t# extract the class ID and confidence (i.e., probability) of\n",
    "\t\t\t# the current object detection\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\t# filter out weak predictions by ensuring the detected\n",
    "\t\t\t# probability is greater than the minimum probability\n",
    "\t\t\tif confidence > CONFIDENCE_THRESHOLD:\n",
    "\t\t\t\t# scale the bounding box coordinates back relative to the\n",
    "\t\t\t\t# size of the image, keeping in mind that YOLO actually\n",
    "\t\t\t\t# returns the center (x, y)-coordinates of the bounding\n",
    "\t\t\t\t# box followed by the boxes' width and height\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t# use the center (x, y)-coordinates to derive the top and\n",
    "\t\t\t\t# and left corner of the bounding box\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\t# update our list of bounding box coordinates, confidences,\n",
    "\t\t\t\t# and class IDs\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\t\t\t\tclassIDs.append(classID)\n",
    "\n",
    "\t# apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "\t# boxes\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD,\n",
    "\t\tCONFIDENCE_THRESHOLD)\n",
    "\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "\t\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\n",
    "\t\t\tcv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\t\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "\t\t\tcv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t\t\t0.5, color, 2)\n",
    "\n",
    "\t# show the output image\n",
    "\tcv2.imshow(\"output\", cv2.resize(image,(800, 600)))\n",
    "\twriter.write(cv2.resize(image,(800, 600)))\n",
    "\tfps.update()\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "fps.stop()\n",
    "\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da388662d4ff4d0926cda7ba9d8d974c72a9c4fd4d160df5d7b7fce3be2b0d62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('tensorflow2.5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
